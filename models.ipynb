{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df74c51c-c075-4c98-a285-27fee264eaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 31553/31553 [03:07<00:00, 168.69it/s]\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv, DataFrame\n",
    "from tqdm import tqdm\n",
    "\n",
    "games_df = read_csv(\"./data/processed_games.csv\")\n",
    "games_df = games_df.drop(['player_result', 'player_start_of_streak','player_streak_id','opponent_result','opponent_start_of_streak','opponent_streak_id'], axis = 1)\n",
    "games_df = games_df.dropna()\n",
    "\n",
    "# Currently all players have a winning result. So we can create new label all assigned with 1\n",
    "games_df[\"label\"] = 1\n",
    "\n",
    "# We want to reflect all data and append it to original. This is required since to train our models, there needs to be 2 different values\n",
    "# for label value. In our case 1 would if player won and 0 would be if they lost.\n",
    "new_games_df = DataFrame()\n",
    "for i, row1 in tqdm(games_df.iterrows(), total=games_df.shape[0]):\n",
    "    new_row = {\n",
    "        \"date\": row1[\"date\"],\n",
    "        \"player\": row1[\"opponent\"],\n",
    "        \"opponent\": row1[\"player\"],\n",
    "        \"PL5G\": row1[\"OL5G\"],\n",
    "        \"OL5G\": row1[\"PL5G\"],\n",
    "        \"PS\": row1[\"OS\"],\n",
    "        \"OS\": row1[\"PS\"],\n",
    "        \"PAW\": row1[\"OAW\"],\n",
    "        \"OAW\": row1[\"PAW\"],\n",
    "        \"PNW\": row1[\"ONW\"],\n",
    "        \"ONW\":row1[\"PNW\"],\n",
    "        \"PNL\": row1[\"ONL\"],\n",
    "        \"ONL\": row1[\"PNL\"],\n",
    "        \"label\": 0,\n",
    "    }\n",
    "    new_games_df = new_games_df.append(new_row, ignore_index=True)\n",
    "\n",
    "games_df = games_df.append(new_games_df,ignore_index=True)\n",
    "\n",
    "games_df[\"PWR\"] = games_df[\"PNW\"] / (games_df[\"PNW\"] + games_df[\"PNL\"])\n",
    "games_df[\"OWR\"] = games_df[\"ONW\"] / (games_df[\"ONW\"] + games_df[\"ONL\"])\n",
    "games_df[\"AWR\"] = games_df[\"PAW\"] / (games_df[\"PAW\"] + games_df[\"OAW\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c53c0b9-546b-4a99-8c6e-20fa8dd4ee0a",
   "metadata": {},
   "source": [
    "#### Modeling\n",
    "\n",
    "After we have processed data, we want to try applying it on several models. Before we dive right in, we want to perform some more actions. Here we will first assign feature columns, label colum then split train and test data.\n",
    "\n",
    "For models, I have personally chosen 3 different models to try. Logistic Regression, Decision Tree Classifier, and Baysian Classifier. The selections were based on one main reason: binary classification method. This project itself needs a model that determines between win or lose, in other words, True or False. This can be considered as a binary classification, and I, at this moment think these 3 are best approaches to make.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6e936b3-c976-462b-8c1b-1e3d79232923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "feature_cols = [\n",
    "    \"PL5G\",\n",
    "    \"OL5G\",\n",
    "    \"PS\",\n",
    "    \"OS\",\n",
    "    \"PWR\",\n",
    "    \"OWR\",\n",
    "    \"AWR\",\n",
    "]\n",
    "\n",
    "X = games_df[feature_cols]\n",
    "y = games_df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32fadbbc-c0fb-42ed-922d-b11dbd4091d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION MATRIX: \n",
      " [[7633  364]\n",
      " [ 351 7429]]\n",
      "ACCURACY: 0.954680864549661\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"CONFUSION MATRIX: \\n\", cnf_matrix)\n",
    "print(\"ACCURACY:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd944583-8fbb-4426-8726-61a9b517eabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION MATRIX:\n",
      " [[7778  219]\n",
      " [ 247 7533]]\n",
      "ACCURACY: 0.9704633326994992\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"CONFUSION MATRIX:\\n\", cnf_matrix)\n",
    "print(\"ACCURACY:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60aa736c-b2c8-41ee-83b2-c360d8cea28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION MATRIX:\n",
      " [[7588  409]\n",
      " [ 392 7388]]\n",
      "ACCURACY: 0.9492298916143753\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"CONFUSION MATRIX:\\n\", cnf_matrix)\n",
    "print(\"ACCURACY:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03d6806-ab4d-46b9-9d47-863ed3cb28a7",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "All three models don't have a significant difference in results. Except right now, Decision Tree seems to have the best fit model. This actually is what I slightly expected. The main reason is, I would predict a player with a win rate against opponent with 80% would be likely to win as well as 90%. In other words, for some features, I hypothesized that if a feature is up to some level, they would be likely to win.\n",
    "\n",
    "One reason I can assume that all 3 models have successfully have a pretty high accuracy is becuase of my sample size. In this project I have a lot of data: probably 30000+ rows. And compared to that I don't have too many features to consider about. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
